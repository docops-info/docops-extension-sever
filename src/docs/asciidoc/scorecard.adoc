include::_base.adoc[]
= Migration ScoreCard Maker
:doctype: book
:icons: font
:source-highlighter: highlightjs
:toc: left
:toclevels: 4
:sectlinks:
:imagesdir: images

== Overview

The Migration ScoreCard Maker is a tool for creating visually appealing scorecards that compare before and after states of a migration project. It's particularly useful for visualizing database migrations, cloud migrations, or any other transformation projects where you want to highlight improvements and key metrics.

== Features

* Dynamic SVG generation based on table-formatted input
* iOS-style design with modern aesthetics
* Customizable themes and colors
* Sections for before and after states
* Performance metrics visualization
* Key optimizations display
* Migration summary with overall improvement score
* Fixed dimensions (800x620 pixels) with scaling options

== Usage

=== Basic Example

[source,asciidoc]
....
[docops,scorecard]
----
title=Database Migration ScoreCard: Oracle ‚Üí AWS Aurora PostgreSQL
subtitle=On-Premise to Cloud Migration with SQL Optimization & Performance Tuning
headerTitle=Database Architecture Transformation & Query Optimization Results
scale=0.6
---
[before]
title=BEFORE: On-Premise Oracle Database
---
[before.items]
Oracle Database 19c (On-Premise) | Physical server, manual scaling, high licensing costs | critical | !
Cross-Datacenter Communication | App in cloud, DB on-premise (high latency) | critical | !
---
[before.performance]
Legacy Performance Baseline | 30 | #e74c3c
---
[after]
title=AFTER: AWS Aurora PostgreSQL
---
[after.items]
Aurora PostgreSQL (AWS Managed) | Auto-scaling, managed service, no licensing fees | good | ‚úì
Same-Region Communication | App and DB both in AWS (low latency) | good | ‚úì
---
[after.performance]
Enhanced Performance | 90 | #27ae60
---
[metrics]
Query Performance Gains | #e74c3c
---
[metrics.items]
Average Query Time | -78%
Full Table Scans | -95%
---
[optimizations]
1 | Removed UPPER() Functions | App handles case conversion, eliminated function overhead
2 | Composite Indexes | Multi-column indexes for complex WHERE clauses
---
[summary]
93 | EXCEPTIONAL | Zero data loss migration | 78% query improvement | 68% cost reduction
---
[footer]
Migration Duration: 8 weeks | Team: 3 DBAs + 2 developers | Downtime: 4 hours
----
....


[docops,scorecard]
----
title=Database Migration ScoreCard: Oracle ‚Üí AWS Aurora PostgreSQL
subtitle=On-Premise to Cloud Migration with SQL Optimization & Performance Tuning
headerTitle=Database Architecture Transformation & Query Optimization Results
scale=0.7
---
[before]
title=BEFORE: On-Premise Oracle Database
---
[before.items]
Oracle Database 19c (On-Premise) | Physical server, manual scaling, high licensing costs | critical | !
Cross-Datacenter Communication | App in cloud, DB on-premise (high latency) | critical | !
---
[before.performance]
Legacy Performance Baseline | 30 | #e74c3c
---
[after]
title=AFTER: AWS Aurora PostgreSQL
---
[after.items]
Aurora PostgreSQL (AWS Managed) | Auto-scaling, managed service, no licensing fees | good | ‚úì
Same-Region Communication | App and DB both in AWS (low latency) | good | ‚úì
---
[after.performance]
Enhanced Performance | 90 | #27ae60
---
[metrics]
Query Performance Gains | #e74c3c
---
[metrics.items]
Average Query Time | -78%
Full Table Scans | -95%
---
[optimizations]
1 | Removed UPPER() Functions | App handles case conversion, eliminated function overhead
2 | Composite Indexes | Multi-column indexes for complex WHERE clauses
---
[summary]
93 | EXCEPTIONAL | Zero data loss migration | 78% query improvement | 68% cost reduction
---
[footer]
Migration Duration: 8 weeks | Team: 3 DBAs + 2 developers | Downtime: 4 hours
----

=== Input Format

The ScoreCard Maker uses a simple table format for input, with sections denoted by `[section]` markers and configuration parameters specified at the beginning.

==== Configuration Parameters

[cols="1,3"]
|===
|Parameter |Description

|title
|The main title of the scorecard

|subtitle
|A subtitle or description

|headerTitle
|The title for the header section (Note: In the iOS-style design, this is used for internal reference but not displayed in the UI)

|scale
|A scaling factor for the SVG (default: 1.0). The base dimensions are 800x620 pixels, which will be multiplied by this factor.

|backgroundColor
|Background color (default: #f8f9fa)

|titleColor
|Title text color (default: #2c3e50)

|subtitleColor
|Subtitle text color (default: #7f8c8d)

|headerColor
|Header background color (default: #8e44ad) (Note: In the iOS-style design, this is used for internal reference but not displayed in the UI)

|beforeSectionColor
|Before section header color (default: #e74c3c)

|afterSectionColor
|After section header color (default: #27ae60)
|===

==== Sections

[cols="1,3"]
|===
|Section |Description

|[before]
|Title for the "before" section

|[before.items]
|Items in the "before" section, format: `Title | Description | Status | Icon`

|[before.performance]
|Performance baseline, format: `Label | Percentage | Color`

|[after]
|Title for the "after" section

|[after.items]
|Items in the "after" section, format: `Title | Description | Status | Icon`

|[after.performance]
|Performance improvement, format: `Label | Percentage | Color`

|[metrics]
|Metrics category, format: `Title | Color`

|[metrics.items]
|Metrics in the category, format: `Label | Value`

|[optimizations]
|Key optimizations, format: `Number | Title | Description`

|[summary]
|Migration summary, format: `Improvement% | Status | Highlight1 | Highlight2 | ...`

|[footer]
|Footer text
|===

==== Status Values

For the `Status` field in `[before.items]` and `[after.items]`:

* `critical` - iOS-style red gradient indicator
* `warning` - iOS-style orange gradient indicator
* `good` - iOS-style green gradient indicator

In the new iOS-style design, these status values are displayed as subtle circular indicators with iOS system color gradients.

==== Icons

Common icons used in the `Icon` field:

* `!` - Converted to ‚ö†Ô∏è (warning emoji) in the iOS-style design
* `$` - Converted to üí∞ (money bag emoji) in the iOS-style design
* `‚úì` - Converted to ‚úÖ (check mark emoji) in the iOS-style design

In the iOS-style design, these text icons are automatically converted to appropriate emojis for a more modern look.

== API

The ScoreCard Maker provides a REST API for generating scorecards:

[cols="1,3"]
|===
|Endpoint |Description

|GET /api/scorecard/edit-mode
|Returns an HTML form for editing the scorecard

|GET /api/scorecard/view-mode
|Returns an HTML view for displaying the scorecard

|PUT /api/scorecard/render
|Processes the form data and returns the rendered SVG
|===

== Examples

=== Database Migration ScoreCard

The example at the beginning of this document shows a Database Migration ScoreCard. Here's another variation focusing on a different database technology:

[source,asciidoc]
....
[docops,scorecard]
----
title=Database Migration ScoreCard: MongoDB ‚Üí Amazon DocumentDB
subtitle=NoSQL Database Migration with Performance Optimization
headerTitle=Document Database Migration Results
scale=1.0
backgroundColor=#f5f5f5
---
[before]
title=BEFORE: Self-Hosted MongoDB
---
[before.items]
MongoDB 4.2 (Self-Hosted) | Manual sharding, complex maintenance, scaling issues | critical | !
Inconsistent Backup Strategy | Weekly backups, manual process, recovery gaps | warning | !
Limited Monitoring | Basic metrics only, no proactive alerts | warning | !
---
[before.performance]
Baseline Performance | 40 | #e74c3c
---
[after]
title=AFTER: Amazon DocumentDB
---
[after.items]
Amazon DocumentDB | Managed service, automatic scaling, MongoDB-compatible | good | ‚úì
Automated Backups | Continuous backups, point-in-time recovery | good | ‚úì
Enhanced Monitoring | CloudWatch integration, custom dashboards | good | ‚úì
---
[after.performance]
Enhanced Performance | 85 | #27ae60
---
[metrics]
Query Performance | #3498db
---
[metrics.items]
Read Operations | -65% latency
Write Operations | -42% latency
Index Utilization | +85% efficiency
---
[optimizations]
1 | Index Optimization | Created compound indexes for common query patterns
2 | Connection Pooling | Implemented proper connection management
3 | Document Structure | Redesigned document schema for better query performance
---
[summary]
88 | EXCELLENT | Zero-downtime migration | 65% latency reduction | 40% cost savings
---
[footer]
Migration Duration: 6 weeks | Team: 2 DBAs + 3 developers | Downtime: None
----
....

[docops,scorecard]
----
title=Database Migration ScoreCard: MongoDB ‚Üí Amazon DocumentDB
subtitle=NoSQL Database Migration with Performance Optimization
headerTitle=Document Database Migration Results
scale=0.7
backgroundColor=#f5f5f5
---
[before]
title=BEFORE: Self-Hosted MongoDB
---
[before.items]
MongoDB 4.2 (Self-Hosted) | Manual sharding, complex maintenance, scaling issues | critical | !
Inconsistent Backup Strategy | Weekly backups, manual process, recovery gaps | warning | !
Limited Monitoring | Basic metrics only, no proactive alerts | warning | !
---
[before.performance]
Baseline Performance | 40 | #e74c3c
---
[after]
title=AFTER: Amazon DocumentDB
---
[after.items]
Amazon DocumentDB | Managed service, automatic scaling, MongoDB-compatible | good | ‚úì
Automated Backups | Continuous backups, point-in-time recovery | good | ‚úì
Enhanced Monitoring | CloudWatch integration, custom dashboards | good | ‚úì
---
[after.performance]
Enhanced Performance | 85 | #27ae60
---
[metrics]
Query Performance | #3498db
---
[metrics.items]
Read Operations | -65% latency
Write Operations | -42% latency
Index Utilization | +85% efficiency
---
[optimizations]
1 | Index Optimization | Created compound indexes for common query patterns
2 | Connection Pooling | Implemented proper connection management
3 | Document Structure | Redesigned document schema for better query performance
---
[summary]
88 | EXCELLENT | Zero-downtime migration | 65% latency reduction | 40% cost savings
---
[footer]
Migration Duration: 6 weeks | Team: 2 DBAs + 3 developers | Downtime: None
----

=== Application Modernization ScoreCard

This example shows how to use the ScoreCard for tracking application modernization efforts:

[source,asciidoc]
....
[docops,scorecard]
----
title=Application Modernization ScoreCard: Monolith to Microservices
subtitle=Legacy Java Application Transformation to Cloud-Native Architecture
headerTitle=Application Architecture Transformation Results
scale=1.0
backgroundColor=#f0f7ff
titleColor=#0a2540
headerColor=#4a56e2
beforeSectionColor=#e63946
afterSectionColor=#2a9d8f
---
[before]
title=BEFORE: Monolithic Java Application
---
[before.items]
Monolithic Architecture | Single codebase, tightly coupled components, difficult to scale | critical | !
Java EE 6 + Oracle | Legacy technology stack with vendor lock-in | critical | !
Manual Deployment | Complex, error-prone deployment process, 2-day release cycle | warning | !
---
[before.performance]
Legacy Application | 35 | #e63946
---
[after]
title=AFTER: Microservices Architecture
---
[after.items]
Microservices | Decoupled services, independent scaling, resilient architecture | good | ‚úì
Spring Boot + Kubernetes | Modern cloud-native stack with containerization | good | ‚úì
CI/CD Pipeline | Automated testing and deployment, multiple releases per day | good | ‚úì
---
[after.performance]
Modernized Application | 92 | #2a9d8f
---
[metrics]
Performance Improvements | #4a56e2
---
[metrics.items]
Response Time | -70%
Throughput | +300%
Resource Utilization | -45%
---
[metrics]
Development Metrics | #4361ee
---
[metrics.items]
Deployment Frequency | Daily (was bi-weekly)
Lead Time for Changes | 2 hours (was 3 days)
Change Failure Rate | 4% (was 23%)
---
[optimizations]
1 | Domain-Driven Design | Restructured services around business domains
2 | Event-Driven Architecture | Implemented message queues for asynchronous processing
3 | API Gateway | Centralized routing, authentication, and rate limiting
4 | Observability | Implemented distributed tracing and centralized logging
---
[summary]
92 | EXCEPTIONAL | Scalable architecture | 70% faster response times | 85% faster releases
---
[footer]
Modernization Duration: 9 months | Team: 12 developers | Approach: Strangler Pattern
----
....

[docops,scorecard]
----
title=Application Modernization ScoreCard: Monolith to Microservices
subtitle=Legacy Java Application Transformation to Cloud-Native Architecture
headerTitle=Application Architecture Transformation Results
scale=0.7
backgroundColor=#f0f7ff
titleColor=#0a2540
headerColor=#4a56e2
beforeSectionColor=#e63946
afterSectionColor=#2a9d8f
---
[before]
title=BEFORE: Monolithic Java Application
---
[before.items]
Monolithic Architecture | Single codebase, tightly coupled components, difficult to scale | critical | !
Java EE 6 + Oracle | Legacy technology stack with vendor lock-in | critical | !
Manual Deployment | Complex, error-prone deployment process, 2-day release cycle | warning | !
---
[before.performance]
Legacy Application | 35 | #e63946
---
[after]
title=AFTER: Microservices Architecture
---
[after.items]
Microservices | Decoupled services, independent scaling, resilient architecture | good | ‚úì
Spring Boot + Kubernetes | Modern cloud-native stack with containerization | good | ‚úì
CI/CD Pipeline | Automated testing and deployment, multiple releases per day | good | ‚úì
---
[after.performance]
Modernized Application | 92 | #2a9d8f
---
[metrics]
Performance Improvements | #4a56e2
---
[metrics.items]
Response Time | -70%
Throughput | +300%
Resource Utilization | -45%
---
[metrics]
Development Metrics | #4361ee
---
[metrics.items]
Deployment Frequency | Daily (was bi-weekly)
Lead Time for Changes | 2 hours (was 3 days)
Change Failure Rate | 4% (was 23%)
---
[optimizations]
1 | Domain-Driven Design | Restructured services around business domains
2 | Event-Driven Architecture | Implemented message queues for asynchronous processing
3 | API Gateway | Centralized routing, authentication, and rate limiting
4 | Observability | Implemented distributed tracing and centralized logging
---
[summary]
92 | EXCEPTIONAL | Scalable architecture | 70% faster response times | 85% faster releases
---
[footer]
Modernization Duration: 9 months | Team: 12 developers | Approach: Strangler Pattern
----
=== DevOps Transformation ScoreCard

This example demonstrates tracking DevOps transformation progress:

[source,asciidoc]
....
[docops,scorecard]
----
title=DevOps Transformation ScoreCard
subtitle=From Traditional Development to DevOps Culture and Practices
headerTitle=Software Delivery Performance Transformation
scale=1.0
backgroundColor=#f8f9fa
headerColor=#6610f2
beforeSectionColor=#dc3545
afterSectionColor=#198754
---
[before]
title=BEFORE: Traditional Development Process
---
[before.items]
Siloed Teams | Separate dev and ops teams with conflicting goals | critical | !
Manual Processes | Manual testing, deployments, and infrastructure management | critical | !
Quarterly Releases | Long release cycles with large batches of changes | warning | !
---
[before.performance]
Delivery Performance | 25 | #dc3545
---
[after]
title=AFTER: DevOps Implementation
---
[after.items]
Cross-Functional Teams | Integrated teams with shared responsibility | good | ‚úì
Automation | CI/CD pipelines, infrastructure as code, automated testing | good | ‚úì
Continuous Delivery | Small, frequent releases with fast feedback | good | ‚úì
---
[after.performance]
Delivery Performance | 88 | #198754
---
[metrics]
DORA Metrics | #6610f2
---
[metrics.items]
Deployment Frequency | 25x per day (was 1x per quarter)
Lead Time for Changes | 2.5 hours (was 45 days)
Mean Time to Recovery | 30 minutes (was 24 hours)
Change Failure Rate | 5% (was 35%)
---
[optimizations]
1 | CI/CD Implementation | Jenkins pipelines with automated testing and deployment
2 | Infrastructure as Code | Terraform for infrastructure provisioning and management
3 | Monitoring & Observability | Prometheus, Grafana, and ELK stack implementation
4 | Feature Flags | Implemented feature toggles for safer deployments
---
[summary]
88 | EXCELLENT | 98% reduction in lead time | 85% reduction in failures | 25x more deployments
---
[footer]
Transformation Duration: 12 months | Approach: Incremental | Training: 24 workshops
----
....

[docops,scorecard]
----
title=DevOps Transformation ScoreCard
subtitle=From Traditional Development to DevOps Culture and Practices
headerTitle=Software Delivery Performance Transformation
scale=1.0
backgroundColor=#f8f9fa
headerColor=#6610f2
beforeSectionColor=#dc3545
afterSectionColor=#198754
---
[before]
title=BEFORE: Traditional Development Process
---
[before.items]
Siloed Teams | Separate dev and ops teams with conflicting goals | critical | !
Manual Processes | Manual testing, deployments, and infrastructure management | critical | !
Quarterly Releases | Long release cycles with large batches of changes | warning | !
---
[before.performance]
Delivery Performance | 25 | #dc3545
---
[after]
title=AFTER: DevOps Implementation
---
[after.items]
Cross-Functional Teams | Integrated teams with shared responsibility | good | ‚úì
Automation | CI/CD pipelines, infrastructure as code, automated testing | good | ‚úì
Continuous Delivery | Small, frequent releases with fast feedback | good | ‚úì
---
[after.performance]
Delivery Performance | 88 | #198754
---
[metrics]
DORA Metrics | #6610f2
---
[metrics.items]
Deployment Frequency | 25x per day (was 1x per quarter)
Lead Time for Changes | 2.5 hours (was 45 days)
Mean Time to Recovery | 30 minutes (was 24 hours)
Change Failure Rate | 5% (was 35%)
---
[optimizations]
1 | CI/CD Implementation | Jenkins pipelines with automated testing and deployment
2 | Infrastructure as Code | Terraform for infrastructure provisioning and management
3 | Monitoring & Observability | Prometheus, Grafana, and ELK stack implementation
4 | Feature Flags | Implemented feature toggles for safer deployments
---
[summary]
88 | EXCELLENT | 98% reduction in lead time | 85% reduction in failures | 25x more deployments
---
[footer]
Transformation Duration: 12 months | Approach: Incremental | Training: 24 workshops
----
=== Website Redesign ScoreCard

This example shows how to track website redesign projects:

[source,asciidoc]
....
[docops,scorecard]
----
title=Website Redesign ScoreCard
subtitle=E-Commerce Platform Modernization and UX Enhancement
headerTitle=Digital Experience Transformation Results
scale=1.0
backgroundColor=#ffffff
headerColor=#6c5ce7
beforeSectionColor=#e84393
afterSectionColor=#00b894
---
[before]
title=BEFORE: Legacy Website
---
[before.items]
Outdated Design | Non-responsive, desktop-only experience | critical | !
Slow Performance | 6.2s average page load time | critical | !
Poor Conversion | 1.2% conversion rate, high bounce rate | warning | !
---
[before.performance]
User Experience Score | 32 | #e84393
---
[after]
title=AFTER: Redesigned Website
---
[after.items]
Modern Design | Responsive, mobile-first approach | good | ‚úì
Optimized Performance | 1.8s average page load time | good | ‚úì
Enhanced Conversion | 3.8% conversion rate, lower bounce rate | good | ‚úì
---
[after.performance]
User Experience Score | 89 | #00b894
---
[metrics]
Performance Metrics | #6c5ce7
---
[metrics.items]
Page Load Time | -71%
Time to Interactive | -68%
First Contentful Paint | -75%
---
[metrics]
Business Metrics | #6c5ce7
---
[metrics.items]
Conversion Rate | +217%
Average Order Value | +28%
Mobile Traffic | +145%
---
[optimizations]
1 | Image Optimization | Implemented WebP format and lazy loading
2 | Code Splitting | Reduced JavaScript bundle size with dynamic imports
3 | Server-Side Rendering | Improved initial load time and SEO
4 | Streamlined Checkout | Reduced checkout steps from 5 to 2
---
[summary]
89 | EXCEPTIONAL | 71% faster loading | 217% higher conversion | 28% higher AOV
---
[footer]
Project Duration: 14 weeks | Team: 3 designers, 4 developers | A/B Testing: 12 experiments
----
....

[docops,scorecard]
----
title=Website Redesign ScoreCard
subtitle=E-Commerce Platform Modernization and UX Enhancement
headerTitle=Digital Experience Transformation Results
scale=1.0
backgroundColor=#ffffff
headerColor=#6c5ce7
beforeSectionColor=#e84393
afterSectionColor=#00b894
---
[before]
title=BEFORE: Legacy Website
---
[before.items]
Outdated Design | Non-responsive, desktop-only experience | critical | !
Slow Performance | 6.2s average page load time | critical | !
Poor Conversion | 1.2% conversion rate, high bounce rate | warning | !
---
[before.performance]
User Experience Score | 32 | #e84393
---
[after]
title=AFTER: Redesigned Website
---
[after.items]
Modern Design | Responsive, mobile-first approach | good | ‚úì
Optimized Performance | 1.8s average page load time | good | ‚úì
Enhanced Conversion | 3.8% conversion rate, lower bounce rate | good | ‚úì
---
[after.performance]
User Experience Score | 89 | #00b894
---
[metrics]
Performance Metrics | #6c5ce7
---
[metrics.items]
Page Load Time | -71%
Time to Interactive | -68%
First Contentful Paint | -75%
---
[metrics]
Business Metrics | #6c5ce7
---
[metrics.items]
Conversion Rate | +217%
Average Order Value | +28%
Mobile Traffic | +145%
---
[optimizations]
1 | Image Optimization | Implemented WebP format and lazy loading
2 | Code Splitting | Reduced JavaScript bundle size with dynamic imports
3 | Server-Side Rendering | Improved initial load time and SEO
4 | Streamlined Checkout | Reduced checkout steps from 5 to 2
---
[summary]
89 | EXCEPTIONAL | 71% faster loading | 217% higher conversion | 28% higher AOV
---
[footer]
Project Duration: 14 weeks | Team: 3 designers, 4 developers | A/B Testing: 12 experiments
----

=== Security Enhancement ScoreCard

This example demonstrates tracking security improvement initiatives:

[source,asciidoc]
....
[docops,scorecard]
----
title=Security Enhancement ScoreCard
subtitle=Enterprise Security Posture Improvement Initiative
headerTitle=Cybersecurity Resilience Transformation
scale=1.0
backgroundColor=#f5f5f5
headerColor=#3a0ca3
beforeSectionColor=#f72585
afterSectionColor=#4cc9f0
---
[before]
title=BEFORE: Security Vulnerabilities
---
[before.items]
Outdated Authentication | Password-only auth, no MFA | critical | !
Unpatched Systems | 68% of systems behind on security patches | critical | !
Limited Monitoring | No 24/7 monitoring, basic logging | warning | !
---
[before.performance]
Security Posture | 28 | #f72585
---
[after]
title=AFTER: Enhanced Security
---
[after.items]
Modern Authentication | SSO with MFA and conditional access | good | ‚úì
Patch Management | Automated patching, 99.5% compliance | good | ‚úì
Advanced Monitoring | 24/7 SOC, SIEM implementation | good | ‚úì
---
[after.performance]
Security Posture | 94 | #4cc9f0
---
[metrics]
Vulnerability Metrics | #3a0ca3
---
[metrics.items]
Critical Vulnerabilities | -98%
Mean Time to Patch | -92%
Security Incidents | -85%
---
[metrics]
Compliance Metrics | #3a0ca3
---
[metrics.items]
Compliance Score | +215%
Audit Findings | -90%
Risk Score | -75%
---
[optimizations]
1 | Zero Trust Architecture | Implemented least privilege access and network segmentation
2 | Automated Security Testing | Integrated SAST/DAST into CI/CD pipeline
3 | Security Awareness Training | Monthly training and simulated phishing campaigns
4 | Endpoint Detection & Response | Deployed EDR solution across all endpoints
---
[summary]
94 | EXCEPTIONAL | 98% fewer vulnerabilities | 85% fewer incidents | 90% fewer audit findings
---
[footer]
Initiative Duration: 10 months | Investment: $1.2M | ROI: 320% (breach prevention)
----
....

[docops,scorecard]
----
title=Security Enhancement ScoreCard
subtitle=Enterprise Security Posture Improvement Initiative
headerTitle=Cybersecurity Resilience Transformation
scale=1.0
backgroundColor=#f5f5f5
headerColor=#3a0ca3
beforeSectionColor=#f72585
afterSectionColor=#4cc9f0
---
[before]
title=BEFORE: Security Vulnerabilities
---
[before.items]
Outdated Authentication | Password-only auth, no MFA | critical | !
Unpatched Systems | 68% of systems behind on security patches | critical | !
Limited Monitoring | No 24/7 monitoring, basic logging | warning | !
---
[before.performance]
Security Posture | 28 | #f72585
---
[after]
title=AFTER: Enhanced Security
---
[after.items]
Modern Authentication | SSO with MFA and conditional access | good | ‚úì
Patch Management | Automated patching, 99.5% compliance | good | ‚úì
Advanced Monitoring | 24/7 SOC, SIEM implementation | good | ‚úì
---
[after.performance]
Security Posture | 94 | #4cc9f0
---
[metrics]
Vulnerability Metrics | #3a0ca3
---
[metrics.items]
Critical Vulnerabilities | -98%
Mean Time to Patch | -92%
Security Incidents | -85%
---
[metrics]
Compliance Metrics | #3a0ca3
---
[metrics.items]
Compliance Score | +215%
Audit Findings | -90%
Risk Score | -75%
---
[optimizations]
1 | Zero Trust Architecture | Implemented least privilege access and network segmentation
2 | Automated Security Testing | Integrated SAST/DAST into CI/CD pipeline
3 | Security Awareness Training | Monthly training and simulated phishing campaigns
4 | Endpoint Detection & Response | Deployed EDR solution across all endpoints
---
[summary]
94 | EXCEPTIONAL | 98% fewer vulnerabilities | 85% fewer incidents | 90% fewer audit findings
---
[footer]
Initiative Duration: 10 months | Investment: $1.2M | ROI: 320% (breach prevention)
----
=== AI/ML Implementation ScoreCard

This example shows how to track AI/ML implementation projects:

[source,asciidoc]
....
[docops,scorecard]
----
title=AI/ML Implementation ScoreCard
subtitle=Customer Service Automation with Machine Learning
headerTitle=Intelligent Automation Transformation Results
scale=1.0
backgroundColor=#f0f0f0
headerColor=#7209b7
beforeSectionColor=#f94144
afterSectionColor=#43aa8b
---
[before]
title=BEFORE: Manual Customer Service
---
[before.items]
Manual Ticket Routing | Human agents manually categorizing tickets | critical | !
No Predictive Capabilities | Reactive approach to customer issues | warning | !
Limited Self-Service | Basic FAQ, no intelligent assistance | warning | !
---
[before.performance]
Service Efficiency | 35 | #f94144
---
[after]
title=AFTER: AI-Enhanced Customer Service
---
[after.items]
ML-Based Ticket Routing | Automatic categorization and priority assignment | good | ‚úì
Predictive Analytics | Proactive issue detection and resolution | good | ‚úì
AI-Powered Chatbot | 24/7 intelligent virtual assistant | good | ‚úì
---
[after.performance]
Service Efficiency | 92 | #43aa8b
---
[metrics]
Operational Metrics | #7209b7
---
[metrics.items]
First Response Time | -72%
Resolution Time | -68%
Agent Productivity | +85%
---
[metrics]
Business Impact | #7209b7
---
[metrics.items]
Customer Satisfaction | +45%
Self-Service Rate | +320%
Cost per Ticket | -62%
---
[optimizations]
1 | NLP Model Training | Custom model trained on historical support data
2 | Sentiment Analysis | Real-time customer sentiment detection
3 | Knowledge Graph | Connected knowledge base for contextual responses
4 | Continuous Learning | Feedback loops for ongoing model improvement
---
[summary]
92 | EXCEPTIONAL | 72% faster responses | 68% faster resolution | 62% lower cost per ticket
---
[footer]
Implementation: 7 months | Data Scientists: 3 | Customer Data: 2.5M historical tickets
----
....

[docops,scorecard]
----
title=AI/ML Implementation ScoreCard
subtitle=Customer Service Automation with Machine Learning
headerTitle=Intelligent Automation Transformation Results
scale=1.0
backgroundColor=#f0f0f0
headerColor=#7209b7
beforeSectionColor=#f94144
afterSectionColor=#43aa8b
---
[before]
title=BEFORE: Manual Customer Service
---
[before.items]
Manual Ticket Routing | Human agents manually categorizing tickets | critical | !
No Predictive Capabilities | Reactive approach to customer issues | warning | !
Limited Self-Service | Basic FAQ, no intelligent assistance | warning | !
---
[before.performance]
Service Efficiency | 35 | #f94144
---
[after]
title=AFTER: AI-Enhanced Customer Service
---
[after.items]
ML-Based Ticket Routing | Automatic categorization and priority assignment | good | ‚úì
Predictive Analytics | Proactive issue detection and resolution | good | ‚úì
AI-Powered Chatbot | 24/7 intelligent virtual assistant | good | ‚úì
---
[after.performance]
Service Efficiency | 92 | #43aa8b
---
[metrics]
Operational Metrics | #7209b7
---
[metrics.items]
First Response Time | -72%
Resolution Time | -68%
Agent Productivity | +85%
---
[metrics]
Business Impact | #7209b7
---
[metrics.items]
Customer Satisfaction | +45%
Self-Service Rate | +320%
Cost per Ticket | -62%
---
[optimizations]
1 | NLP Model Training | Custom model trained on historical support data
2 | Sentiment Analysis | Real-time customer sentiment detection
3 | Knowledge Graph | Connected knowledge base for contextual responses
4 | Continuous Learning | Feedback loops for ongoing model improvement
---
[summary]
92 | EXCEPTIONAL | 72% faster responses | 68% faster resolution | 62% lower cost per ticket
---
[footer]
Implementation: 7 months | Data Scientists: 3 | Customer Data: 2.5M historical tickets
----
=== Cloud Migration ScoreCard

This example demonstrates tracking cloud migration projects:

[source,asciidoc]
....
[docops,scorecard]
----
title=Cloud Migration ScoreCard: On-Premise to AWS
subtitle=Enterprise Infrastructure Transformation to Cloud-Native Architecture
headerTitle=Digital Infrastructure Modernization Results
scale=1.0
backgroundColor=#f7f7f7
headerColor=#0077b6
beforeSectionColor=#ef476f
afterSectionColor=#06d6a0
---
[before]
title=BEFORE: On-Premise Data Center
---
[before.items]
Physical Infrastructure | Hardware refresh cycles, capital expenses | critical | $
Limited Scalability | Manual scaling, capacity planning challenges | warning | !
High Maintenance | 65% of IT time spent on "keeping the lights on" | critical | !
---
[before.performance]
Infrastructure Agility | 30 | #ef476f
---
[after]
title=AFTER: AWS Cloud Environment
---
[after.items]
Cloud Infrastructure | Pay-as-you-go model, operational expenses | good | $
Auto-Scaling | Dynamic resource allocation based on demand | good | ‚úì
Managed Services | 70% reduction in maintenance overhead | good | ‚úì
---
[after.performance]
Infrastructure Agility | 88 | #06d6a0
---
[metrics]
Performance Metrics | #0077b6
---
[metrics.items]
Provisioning Time | -95% (days to minutes)
System Availability | +2.1% (99.9% to 99.99%)
Disaster Recovery | RTO reduced by 85%
---
[metrics]
Financial Metrics | #0077b6
---
[metrics.items]
Infrastructure Costs | -32%
Operational Overhead | -65%
Innovation Capacity | +120%
---
[optimizations]
1 | Infrastructure as Code | Terraform for automated provisioning
2 | Containerization | Docker and Kubernetes for application deployment
3 | Serverless Architecture | Lambda functions for event-driven workloads
4 | Cloud-Native Security | Defense-in-depth security architecture
---
[summary]
88 | EXCELLENT | 32% cost reduction | 95% faster provisioning | 65% less operational overhead
---
[footer]
Migration Duration: 8 months | Workloads: 85 applications | Approach: 6R Strategy
----
....

[docops,scorecard]
----
title=Cloud Migration ScoreCard: On-Premise to AWS
subtitle=Enterprise Infrastructure Transformation to Cloud-Native Architecture
headerTitle=Digital Infrastructure Modernization Results
scale=0.7
backgroundColor=#f7f7f7
headerColor=#0077b6
beforeSectionColor=#ef476f
afterSectionColor=#06d6a0
---
[before]
title=BEFORE: On-Premise Data Center
---
[before.items]
Physical Infrastructure | Hardware refresh cycles, capital expenses | critical | $
Limited Scalability | Manual scaling, capacity planning challenges | warning | !
High Maintenance | 65% of IT time spent on "keeping the lights on" | critical | !
---
[before.performance]
Infrastructure Agility | 30 | #ef476f
---
[after]
title=AFTER: AWS Cloud Environment
---
[after.items]
Cloud Infrastructure | Pay-as-you-go model, operational expenses | good | $
Auto-Scaling | Dynamic resource allocation based on demand | good | ‚úì
Managed Services | 70% reduction in maintenance overhead | good | ‚úì
---
[after.performance]
Infrastructure Agility | 88 | #06d6a0
---
[metrics]
Performance Metrics | #0077b6
---
[metrics.items]
Provisioning Time | -95% (days to minutes)
System Availability | +2.1% (99.9% to 99.99%)
Disaster Recovery | RTO reduced by 85%
---
[metrics]
Financial Metrics | #0077b6
---
[metrics.items]
Infrastructure Costs | -32%
Operational Overhead | -65%
Innovation Capacity | +120%
---
[optimizations]
1 | Infrastructure as Code | Terraform for automated provisioning
2 | Containerization | Docker and Kubernetes for application deployment
3 | Serverless Architecture | Lambda functions for event-driven workloads
4 | Cloud-Native Security | Defense-in-depth security architecture
---
[summary]
88 | EXCELLENT | 32% cost reduction | 95% faster provisioning | 65% less operational overhead
---
[footer]
Migration Duration: 8 months | Workloads: 85 applications | Approach: 6R Strategy
----
== Best Practices and Creative Uses

=== Best Practices for Creating Effective Scorecards

When creating your own scorecards, consider these best practices:

1. **Focus on Contrast**: Ensure a clear contrast between "before" and "after" states to highlight improvements.
2. **Use Consistent Metrics**: Choose metrics that can be directly compared between the before and after states.
3. **Limit Items**: Include 3-5 key items in each section to avoid overwhelming the viewer.
4. **Choose Meaningful Colors**: Use colors that intuitively represent status (red for critical, green for good).
5. **Quantify Improvements**: Express improvements in percentages or other measurable terms.
6. **Highlight Key Optimizations**: Focus on the most impactful changes that drove the improvements.
7. **Include Context in Footer**: Add relevant information about timeline, team size, or approach in the footer.
8. **Be Specific in Titles**: Use specific, descriptive titles rather than generic ones.

=== Creative Uses for ScoreCards

Beyond the examples shown above, ScoreCards can be used for:

1. **Product Launch Comparisons**: Compare metrics between previous and new product versions.
2. **Team Performance Tracking**: Visualize team performance improvements over time.
3. **User Experience Enhancements**: Document UX improvements with before/after metrics.
4. **Infrastructure Upgrades**: Showcase the impact of hardware or network upgrades.
5. **Training Program Effectiveness**: Compare skill levels before and after training initiatives.
6. **Marketing Campaign Results**: Display the impact of marketing strategy changes.
7. **Software Version Comparisons**: Highlight improvements between software releases.
8. **Process Optimization**: Document the results of business process reengineering.
9. **Compliance Improvements**: Track progress in meeting regulatory requirements.
10. **Sustainability Initiatives**: Showcase environmental impact reductions.

=== iOS-Style Design

The ScoreCard Maker now uses an iOS-style design with the following elements:

1. **Fixed Dimensions**: Each scorecard has fixed dimensions of 800x620 pixels, which can be scaled using the `scale` parameter.
2. **Subtle Background Gradient**: A light gradient background for a modern, clean look.
3. **Card-Based Layout**: Content is organized in card-like containers with rounded corners.
4. **iOS-Style Shadows**: Subtle drop shadows add depth and dimension to the cards.
5. **System Gradients**: iOS-style color gradients for headers and indicators (blue, green, red, orange).
6. **Modern Typography**: Uses the San Francisco font family (or system equivalents) for a native iOS feel.
7. **Team Avatars**: Includes a section for team member avatars with initials.
8. **Key Improvements Summary**: A dedicated section highlighting the overall improvements.

=== Design Tips

1. **Color Harmony**: Choose complementary colors for a visually appealing scorecard.
2. **Typography**: The default typography is designed for readability, but you can customize it if needed.
3. **White Space**: The layout includes appropriate white space for readability.
4. **Visual Hierarchy**: The most important information (summary and key metrics) is emphasized.
5. **Consistency**: Maintain consistent formatting throughout your scorecard.

== Customization

The ScoreCard Maker supports customization through various parameters:

* Colors for different sections
* Scaling for different display sizes (base dimensions: 800x620 pixels)
* Custom icons and status indicators
* iOS-style visual elements (gradients, shadows, rounded corners)
* Fixed layout with consistent design elements

== Integration

The ScoreCard Maker can be integrated with other documentation tools:

* Embed in AsciiDoc documents
* Include in HTML pages
* Export as SVG for use in presentations
* Convert to PDF for reports

== Quick Reference Guide

=== Common Patterns

Here are some common patterns you can use as templates for your own scorecards:

==== Status Indicators

[cols="1,1,2"]
|===
|Status |Icon |Use Case

|critical
|!
|Highlighting problems or critical issues in the "before" state

|warning
|!
|Indicating caution or moderate issues

|good
|‚úì
|Showing improvements or positive aspects in the "after" state

|neutral
|‚Ä¢
|Presenting neutral information without judgment
|===

==== Metric Formats

[cols="2,2"]
|===
|Format |Example

|Percentage decrease
|-75%

|Percentage increase
|+120%

|Before/After comparison
|2.5s ‚Üí 0.8s

|Scale comparison
|3/10 ‚Üí 9/10

|Ratio improvement
|1:5 ‚Üí 1:20
|===

==== Color Schemes

[cols="1,1,2"]
|===
|Theme |Colors |Use Case

|Corporate
|#0a2540, #4a56e2, #2a9d8f
|Professional business presentations

|High Contrast
|#000000, #ffffff, #ff0000, #00ff00
|Maximum readability and accessibility

|Pastel
|#f8edeb, #fcd5ce, #f9dcc4, #fec89a
|Softer, more approachable look

|Vibrant
|#f72585, #7209b7, #3a0ca3, #4361ee
|Eye-catching presentations with high impact
|===

==== Template Snippets

**Basic Before/After Item:**
[source,text]
----
[before.items]
Item Name | Description of the issue | critical | !
---
[after.items]
Improved Item | Description of the improvement | good | ‚úì
----

**Performance Metrics:**
[source,text]
----
[metrics]
Performance Metrics | #3498db
---
[metrics.items]
Response Time | -65%
Throughput | +120%
Error Rate | -85%
----

**Optimization Entry:**
[source,text]
----
[optimizations]
1 | Optimization Title | Detailed description of what was done and why it helped
----

**Summary Line:**
[source,text]
----
[summary]
85 | EXCELLENT | Key highlight 1 | Key highlight 2 | Key highlight 3
----

== Troubleshooting

=== Common Issues

* If sections are not appearing correctly, check that you have the correct section markers (`[section]`)
* If colors are not displaying correctly, ensure they are valid hex color codes (e.g., `#3498db`)
* If the layout seems off, try adjusting the `scale` parameter
* If metrics aren't showing up, verify that you have both a `[metrics]` section and a corresponding `[metrics.items]` section
* If the scorecard appears too small or too large, remember that the base dimensions are fixed at 800x620 pixels, which are then multiplied by the `scale` parameter
* If icons don't appear as expected, note that in the iOS-style design, text icons (`!`, `$`, `‚úì`) are automatically converted to emojis

=== Support

For issues or feature requests, please contact the DocOps team.
